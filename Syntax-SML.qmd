---
title: "Syntax Model"
format: 
  html:
    embed-resources: true
    toc: true
    self-contained: true
---

# Generate Data
```{r}
library(MASS)
```

```{r}
set.seed(50)

n <- 200  # Jumlah observasi
p <- 10   # Jumlah prediktor

# Generate covariate matrix dengan korelasi
library(MASS)
cov_matrix <- diag(p) * 1.1^2 + matrix(0.3, p, p)  # Menambahkan korelasi antar variabel
mu <- rep(2.5, p)
X <- mvrnorm(n, mu, cov_matrix)
X <- data.frame(X)
colnames(X) <- paste0("X", 1:p)

# Efek linear & nonlinear
beta <- rep(0, p)
beta_0 <- 2
beta[1:5] <- 1.25  # Linear relationship
beta[6:10] <- 0.6   # Nonlinear relationship

# Tambahkan interaksi antar variabel
interact_term <- X$X1 * X$X2 + X$X3 * X$X4

# Tambahkan efek polinomial
poly_term <- X$X5^2 + X$X6^3

# Heteroskedastisitas: error tergantung pada X
error <- rnorm(n, 0, 1.5 * abs(X$X1))

# Buat Y dengan semua efek
Y <- beta_0 + as.matrix(X[, 1:5]) %*% beta[1:5] + 
     sin(as.matrix(X[, 6:10]) %*% beta[6:10]) +
     0.8 * interact_term + 0.5 * poly_term + error

data <- data.frame(Y, X)

# Tambahkan beberapa outlier untuk menguji ketahanan model
outlier_idx <- sample(1:n, size = 5)
data$Y[outlier_idx] <- data$Y[outlier_idx] + rnorm(5, mean = 10, sd = 5)

library(dplyr)
glimpse(data)
```

## Train-Test Split
```{r}
set.seed(50)
train_index <- sample(1:n, size = 0.8 * n, replace = FALSE)
train_data <- data[train_index, ] 
test_data  <- data[-train_index, ]

X_train <- as.matrix(train_data[, -1])
y_train <- train_data$Y
X_test <- as.matrix(test_data[, -1])  
y_test <- test_data$Y
```

# Linear Regression
```{r}
lm_model <- lm(Y ~ ., data = train_data)
summary(lm_model)
```

```{r}
pred_lm <- predict(lm_model, test_data)
mse_lm <- mean((y_test - pred_lm)^2)
rsq_lm <- 1 - sum((y_test - pred_lm)^2) / sum((y_test - mean(y_test))^2)
```

```{r}
library(ggplot2)

plot_data <- data.frame(Actual = test_data$Y, Predicted = pred_lm)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 1.5, color = "black") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Nilai Aktual",
       y = "Nilai Prediksi") +
  annotate("text", x = min(plot_data$Actual), y = max(plot_data$Predicted), 
           label = paste("R =", round(rsq_lm, 3)), hjust = 0, size = 5, color = "black") +
  theme_minimal()
```


# Subset Selection Model

## Forward Selection
```{r}
library(MASS)
```


```{r}
full_model <- lm(Y ~ ., data = train_data)
null_model <- lm(Y ~ 1, data = train_data)

step_forward <- stepAIC(null_model, scope = list(lower = null_model, upper = full_model), 
                         direction = "forward", trace = FALSE)
summary(step_forward)
```

```{r}
pred_forward <- predict(step_forward, test_data)
mse_forward <- mean((y_test - pred_forward)^2)
rsq_forward <- 1 - sum((y_test - pred_forward)^2) / sum((y_test - mean(y_test))^2)
```

```{r}
plot_data <- data.frame(Actual = test_data$Y, Predicted = pred_forward)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 1.5, color = "black") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Nilai Aktual",
       y = "Nilai Prediksi") +
  annotate("text", x = min(plot_data$Actual), y = max(plot_data$Predicted), 
           label = paste("R =", round(rsq_forward, 3)), hjust = 0, size = 5, color = "black") +
  theme_minimal()
```


## Backward Elimination
```{r}
step_backward <- stepAIC(full_model, direction = "backward", trace = FALSE)
summary(step_backward)
```

```{r}
pred_backward <- predict(step_backward, test_data)
mse_backward <- mean((y_test - pred_backward)^2)
rsq_backward <- 1 - sum((y_test - pred_backward)^2) / sum((y_test - mean(y_test))^2)
```


```{r}
plot_data <- data.frame(Actual = test_data$Y, Predicted = pred_backward)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 1.5, color = "black") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Nilai Aktual",
       y = "Nilai Prediksi") +
  annotate("text", x = min(plot_data$Actual), y = max(plot_data$Predicted), 
           label = paste("R =", round(rsq_backward, 3)), hjust = 0, size = 5, color = "black") +
  theme_minimal()
```


## Stepwise Regression
```{r}
step_stepwise <- stepAIC(null_model, scope = list(lower = null_model, upper = full_model), 
                          direction = "both", trace = FALSE)
summary(step_stepwise)
```

```{r}
pred_stepwise <- predict(step_stepwise, test_data)
mse_stepwise <- mean((y_test - pred_stepwise)^2)
rsq_stepwise <- 1 - sum((y_test - pred_stepwise)^2) / sum((y_test - mean(y_test))^2)
```

```{r}
plot_data <- data.frame(Actual = test_data$Y, Predicted = pred_stepwise)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 1.5, color = "black") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Nilai Aktual",
       y = "Nilai Prediksi") +
  annotate("text", x = min(plot_data$Actual), y = max(plot_data$Predicted), 
           label = paste("R =", round(rsq_stepwise, 3)), hjust = 0, size = 5, color = "black") +
  theme_minimal()
```


## Best-Subset Selection
```{r}
library(leaps)
```


```{r}
best_subset <- regsubsets(Y ~ ., data = train_data, nvmax = 10, really.big = TRUE)
best_model_idx <- which.max(summary(best_subset)$adjr2)
best_vars <- names(coef(best_subset, best_model_idx))[-1]

best_subset_model <- lm(as.formula(paste("Y ~", paste(best_vars, collapse = " + "))), data = train_data)
summary(best_subset_model)
```

```{r}
pred_best_subset <- predict(best_subset_model, test_data)
mse_best_subset <- mean((y_test - pred_best_subset)^2)
rsq_best_subset <- 1 - sum((y_test - pred_best_subset)^2) / sum((y_test - mean(y_test))^2)
```

```{r}
plot_data <- data.frame(Actual = test_data$Y, Predicted = pred_best_subset)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 1.5, color = "black") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Nilai Aktual",
       y = "Nilai Prediksi") +
  annotate("text", x = min(plot_data$Actual), y = max(plot_data$Predicted), 
           label = paste("R =", round(rsq_best_subset, 3)), hjust = 0, size = 5, color = "black") +
  theme_minimal()
```


# Shrinkage Methods

## Ridge Regression
```{r}
library(glmnet)
```


```{r}
ridge <- cv.glmnet(X_train, 
                   y_train, 
                   alpha = 0, 
                   type.measure = "mse", 
                   family = "gaussian", 
                   nfolds = 10)

best_lambda_ridge <- ridge$lambda.min ; best_lambda_ridge
```

```{r}
plot(ridge)
```

```{r}
pred_ridge <- predict(ridge, s = best_lambda_ridge, newx = X_test)
mse_ridge <- mean((y_test - pred_ridge)^2)
rsq_ridge <- 1 - sum((y_test - pred_ridge)^2) / sum((y_test - mean(y_test))^2)
```

```{r}
pred_ridge <- as.vector(predict(ridge, s = best_lambda_ridge, newx = X_test))
plot_data <- data.frame(Actual = test_data$Y, Predicted = pred_ridge)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 1.5, color = "black") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Nilai Aktual",
       y = "Nilai Prediksi") +
  annotate("text", x = min(plot_data$Actual), y = max(plot_data$Predicted), 
           label = paste("R =", round(rsq_ridge, 3)), hjust = 0, size = 5, color = "black") +
  theme_minimal()
```


## LASSO Regression
```{r}
lasso <- cv.glmnet(X_train, 
                   y_train, 
                   alpha = 1, 
                   type.measure = "mse", 
                   family = "gaussian", 
                   nfolds = 10)

best_lambda_lasso <- lasso$lambda.min ; best_lambda_lasso
```
```{r}
plot(lasso)
```

```{r}
pred_lasso <- predict(lasso, s = best_lambda_lasso, newx = X_test)
mse_lasso <- mean((y_test - pred_lasso)^2)
rsq_lasso <- 1 - sum((y_test - pred_lasso)^2) / sum((y_test - mean(y_test))^2)
```

```{r}
pred_lasso <- as.vector(predict(ridge, s = best_lambda_lasso, newx = X_test))
plot_data <- data.frame(Actual = test_data$Y, Predicted = pred_lasso)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 1.5, color = "black") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Nilai Aktual",
       y = "Nilai Prediksi") +
  annotate("text", x = min(plot_data$Actual), y = max(plot_data$Predicted), 
           label = paste("R =", round(rsq_lasso, 3)), hjust = 0, size = 5, color = "black") +
  theme_minimal()
```

## Elastic Net
```{r}
elastic_net <- cv.glmnet(X_train, 
                         y_train, 
                         alpha = 0.5, 
                         type.measure = "mse", 
                         family = "gaussian", 
                         nfolds = 10)

best_lambda_en <- elastic_net$lambda.min ; best_lambda_en 
```

```{r}
plot(elastic_net)
```


```{r}
pred_en <- predict(elastic_net, s = best_lambda_en, newx = X_test)
mse_en <- mean((y_test - pred_en)^2)
rsq_en <- 1 - sum((y_test - pred_en)^2) / sum((y_test - mean(y_test))^2)
```

```{r}
pred_en <- as.vector(predict(ridge, s = best_lambda_en, newx = X_test))
plot_data <- data.frame(Actual = test_data$Y, Predicted = pred_lasso)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 1.5, color = "black") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Nilai Aktual",
       y = "Nilai Prediksi") +
  annotate("text", x = min(plot_data$Actual), y = max(plot_data$Predicted), 
           label = paste("R =", round(rsq_en, 3)), hjust = 0, size = 5, color = "black") +
  theme_minimal()
```

# Non-Linear Relationship Model

## Regresi Polinomial
```{r}
# Mean Squared Error Function
mse <- function(model, data) {
  preds <- predict(model, newdata = data)
  mean((data$Y - preds)^2)
}

# Cross-Validation Function for Polynomial Regression
cv_error_poly <- function(degree, data, folds = 5) {
  n <- nrow(data)
  set.seed(123)
  folds_index <- sample(rep(1:folds, length.out = n))
  errors <- numeric(folds)
  
  for (i in 1:folds) {
    train_data <- data[folds_index != i, ]
    test_data  <- data[folds_index == i, ]
    
    formula_poly <- as.formula(paste("Y ~", paste(paste0("poly(X", 1:10, ", ", degree, ")"), collapse = " + ")))
    
    model <- lm(formula_poly, data = train_data)
    
    errors[i] <- sqrt(mse(model, test_data))
  }
  
  return(mean(errors))
}

# Perform Cross-Validation for Different Polynomial Degrees
degrees <- 1:5
cv_errors <- sapply(degrees, function(degree) cv_error_poly(degree, train_data))
best_degree <- degrees[which.min(cv_errors)]

# Create Results Table
results_table <- data.frame(
  Derajat = degrees,
  RMSE = cv_errors,
  Derajat_Terbaik = ifelse(degrees == best_degree, "Yes", "No")
)

# Print Table
print(results_table, row.names = FALSE)
```

```{r}
degree <- 1 #misal 1
formula_poly <- as.formula(paste("Y ~", paste(paste0("poly(", names(train_data)[-1], ", ", degree, ")"), collapse = " + ")))

poly_model <- lm(formula_poly, data = train_data)
summary(poly_model)
```

```{r}
x_var <- "X1"
plot_data <- data.frame(X = train_data[[x_var]],  Y = train_data$Y)

plot_data$Predicted_Y <- predict(poly_model, newdata = train_data)

ggplot(plot_data, aes(x = X, y = Y)) +
  geom_point(alpha = 0.5, color = "blue") +  
  geom_line(aes(y = Predicted_Y), color = "black", size = 1) +
  labs(x = x_var,
       y = "Y") +
  theme_minimal()

```


```{r}
pred_poly <- predict(poly_model, test_data)
mse_poly <- mean((y_test - pred_poly)^2)
rsq_poly <- 1 - sum((y_test - pred_poly)^2) / sum((y_test - mean(y_test))^2)
```

```{r}
plot_data <- data.frame(Actual = test_data$Y, Predicted = pred_poly)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 1.5, color = "black") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Nilai Aktual",
       y = "Nilai Prediksi") +
  annotate("text", x = min(plot_data$Actual), y = max(plot_data$Predicted), 
           label = paste("R =", round(rsq_poly, 3)), hjust = 0, size = 5, color = "black") +
  theme_minimal()
```

## Fungsi Tangga 
```{r}
library(dplyr)
```


```{r}
# Mean Squared Error Function
mse <- function(model, data) {
  preds <- predict(model, newdata = data)
  mean((data$Y - preds)^2)
}

# Cross-Validation Function for Piecewise Constant Regression
cv_error_piecewise <- function(n_bins, data, folds = 5) {
  n <- nrow(data)
  set.seed(123)
  folds_index <- sample(rep(1:folds, length.out = n))
  errors <- numeric(folds)
  
  for (i in 1:folds) {
    train_data <- data[folds_index != i, ]
    test_data  <- data[folds_index == i, ]
    
    # Discretize each X variable into n_bins categories
    for (j in 1:10) {
      train_data[[paste0("X", j, "_bin")]] <- cut(train_data[[paste0("X", j)]], breaks = n_bins, labels = FALSE)
      test_data[[paste0("X", j, "_bin")]] <- cut(test_data[[paste0("X", j)]], breaks = n_bins, labels = FALSE)
    }
    
    # Build the formula
    formula_piecewise <- as.formula(paste("Y ~", paste(paste0("factor(X", 1:10, "_bin)"), collapse = " + ")))
    
    model <- lm(formula_piecewise, data = train_data)
    
    errors[i] <- sqrt(mse(model, test_data))
  }
  
  return(mean(errors))
}

# Perform Cross-Validation for Different Number of Bins
bins <- 2:5
cv_errors <- sapply(bins, function(n_bins) cv_error_piecewise(n_bins, train_data))
best_bins <- bins[which.min(cv_errors)]

# Create Results Table
results_table <- data.frame(
  Jumlah_Interval = bins,
  RMSE = cv_errors,
  Interval_Terbaik = ifelse(bins == best_bins, "Yes", "No")
)

# Print Table
print(results_table, row.names = FALSE)
```


```{r}
for (j in 1:10) {
  train_data[[paste0("X", j, "_bin")]] <- cut(train_data[[paste0("X", j)]], breaks = best_bins, labels = FALSE)
  test_data[[paste0("X", j, "_bin")]] <- cut(test_data[[paste0("X", j)]], breaks = best_bins, labels = FALSE)
}

formula_piecewise <- as.formula(paste("Y ~", paste(paste0("factor(X", 1:10, "_bin)"), collapse = " + ")))
piecewise_model <- lm(formula_piecewise, data = train_data)

# Model summary
summary(piecewise_model)
```

```{r}
x_var <- "X1"
plot_data <- data.frame(X = train_data[[x_var]],  Y = train_data$Y)

plot_data$Predicted_Y <- predict(piecewise_model, newdata = train_data)

ggplot(plot_data, aes(x = X, y = Y)) +
  geom_point(alpha = 0.5, color = "blue") +  
  geom_line(aes(y = Predicted_Y), color = "black", size = 1) +
  labs(x = x_var,
       y = "Y") +
  theme_minimal()
```


```{r}
pred_piecewise <- predict(piecewise_model, test_data)
mse_piecewise <- mean((y_test - pred_piecewise)^2)
rsq_piecewise <- 1 - sum((y_test - pred_piecewise)^2) / sum((y_test - mean(y_test))^2)
```

```{r}
plot_data <- data.frame(Actual = test_data$Y, Predicted = pred_piecewise)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 1.5, color = "black") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Nilai Aktual",
       y = "Nilai Prediksi") +
  annotate("text", x = min(plot_data$Actual), y = max(plot_data$Predicted), 
           label = paste("R =", round(rsq_piecewise, 3)), hjust = 0, size = 5, color = "black") +
  theme_minimal()
```

## Regresi Spline
```{r}
library(splines)
```


```{r}
# Mean Squared Error Function
mse <- function(model, data) {
  preds <- predict(model, newdata = data)
  mean((data$Y - preds)^2)
}

# Cross-Validation Function for Spline Regression
cv_error_spline <- function(n_knots, data, folds = 5) {
  n <- nrow(data)
  set.seed(123)
  folds_index <- sample(rep(1:folds, length.out = n))
  errors <- numeric(folds)
  
  for (i in 1:folds) {
    train_data <- data[folds_index != i, ]
    test_data  <- data[folds_index == i, ]
    
    # Buat formula regresi spline dengan bs() (basis spline)
    formula_spline <- as.formula(paste("Y ~", paste(paste0("bs(X", 1:10, ", df = ", n_knots, ")"), collapse = " + ")))
    
    model <- lm(formula_spline, data = train_data)
    
    errors[i] <- sqrt(mse(model, test_data))
  }
  
  return(mean(errors))
}

# Perform Cross-Validation for Different Number of Knots
knots <- 2:5
cv_errors_spline <- sapply(knots, function(n_knots) cv_error_spline(n_knots, train_data))
best_knots <- knots[which.min(cv_errors_spline)]

# Create Results Table
results_table <- data.frame(
  Jumlah_Knots = knots,
  RMSE = cv_errors_spline,
  Knots_Terbaik = ifelse(knots == best_knots, "Yes", "No")
)

# Print Table
print(results_table, row.names = FALSE)

```

```{r}
# Train final spline model
formula_spline <- as.formula(paste("Y ~", paste(paste0("bs(X", 1:10, ", df = ", best_knots, ")"), collapse = " + ")))

spline_model <- lm(formula_spline, data = train_data)

# Model summary
summary(spline_model)
```

```{r}
x_var <- "X1"
plot_data <- data.frame(X = train_data[[x_var]],  Y = train_data$Y)

plot_data$Predicted_Y <- predict(spline_model, newdata = train_data)

ggplot(plot_data, aes(x = X, y = Y)) +
  geom_point(alpha = 0.5, color = "blue") +  
  geom_line(aes(y = Predicted_Y), color = "black", size = 1) +
  labs(x = x_var,
       y = "Y") +
  theme_minimal()
```

```{r}
pred_spline <- predict(spline_model, test_data)
mse_spline <- mean((y_test - pred_spline)^2)
rsq_spline <- 1 - sum((y_test - pred_spline)^2) / sum((y_test - mean(y_test))^2)
```

```{r}
plot_data <- data.frame(Actual = test_data$Y, Predicted = pred_spline)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 1.5, color = "black") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Nilai Aktual",
       y = "Nilai Prediksi") +
  annotate("text", x = min(plot_data$Actual), y = max(plot_data$Predicted), 
           label = paste("R =", round(rsq_spline, 3)), hjust = 0, size = 5, color = "black") +
  theme_minimal()
```

## GAM
```{r}
library(mgcv)

formula_gam <- as.formula(paste("Y ~", paste(paste0("s(X", 1:10, ", k = 5)"), collapse = " + ")))
gam_model <- gam(formula_gam, data = train_data)
summary(gam_model)
```


```{r}
pred_gam <- predict(gam_model, test_data)
mse_gam <- mean((y_test - pred_gam)^2)
rsq_gam <- 1 - sum((y_test - pred_gam)^2) / sum((y_test - mean(y_test))^2)
```

```{r}
plot_data <- data.frame(Actual = test_data$Y, Predicted = pred_gam)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 1.5, color = "black") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Nilai Aktual",
       y = "Nilai Prediksi") +
  annotate("text", x = min(plot_data$Actual), y = max(plot_data$Predicted), 
           label = paste("R =", round(rsq_gam, 3)), hjust = 0, size = 5, color = "black") +
  theme_minimal()
```

# Machine Learning Model

## Regression Tree

```{r}
library(rpart)
library(rpart.plot)
```


```{r}
# Build regression tree model
formula_tree <- as.formula("Y ~ .")
tree_model <- rpart(formula_tree, data = train_data, method = "anova",
                    control = rpart.control(
                      minsplit = 10,    # Minimum 10 data untuk split
                      minbucket = 5,    # Minimum 5 data per leaf
                      cp = 0.01,        # Complexity Parameter (lebih kecil = lebih kompleks)
                      maxdepth = 5,     # Maksimal kedalaman pohon
                      xval = 10         # Cross-validation folds
                    ))

# Visualisasi Tree
rpart.plot(tree_model, main = "Regression Tree")
```


```{r}
# Cari nilai CP terbaik dari tabel kompleksitas
best_cp <- tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"]

# Prune pohon menggunakan CP terbaik
pruned_tree <- prune(tree_model, cp = best_cp)

# Visualisasi pohon setelah pruning
rpart.plot(pruned_tree, main = "Pruned Regression Tree")
```

```{r}
# Predict on test data
pred_tree <- predict(tree_model, test_data)
pred_pruned <- predict(pruned_tree, test_data)

# Compute MSE
mse_tree <- mean((y_test - pred_tree)^2)
mse_pruned <- mean((y_test - pred_pruned)^2)

# Compute R-squared
rsq_tree <- 1 - sum((y_test - pred_tree)^2) / sum((y_test - mean(y_test))^2)
rsq_pruned <- 1 - sum((y_test - pred_pruned)^2) / sum((y_test - mean(y_test))^2)
```

## Random Forest
```{r}
library(randomForest)
```


```{r}
# Definisikan hyperparameter grid
ntree_vals <- c(100, 300, 500)
mtry_vals <- c(2, sqrt(ncol(train_data) - 1), ncol(train_data) - 1)
nodesize_vals <- c(1, 5, 10)
maxnodes_vals <- c(10, 30, 50)  # Menentukan jumlah maksimal leaf nodes
maxdepth_vals <- c(5, 10, 15)   # Menentukan kedalaman maksimum pohon

# Simpan hasil tuning
results <- expand.grid(ntree = ntree_vals, mtry = mtry_vals, nodesize = nodesize_vals,
                       maxnodes = maxnodes_vals, maxdepth = maxdepth_vals, MSE = NA)

formula_rf <- as.formula("Y ~ .")

# Loop untuk mencari kombinasi terbaik
for (i in 1:nrow(results)) {
  set.seed(123)
  rf_tuned <- randomForest(formula_rf, data = train_data,
                           ntree = results$ntree[i],
                           mtry = results$mtry[i],
                           nodesize = results$nodesize[i],
                           maxnodes = results$maxnodes[i],
                           maxdepth = results$maxdepth[i])
  
  # Prediksi di test data
  pred_tuned <- predict(rf_tuned, test_data)
  
  # Hitung MSE
  results$MSE[i] <- mean((y_test - pred_tuned)^2)
}

# Cari kombinasi terbaik
best_params <- results[which.min(results$MSE), ]
print(best_params)
```

```{r}
# Gunakan hasil dari tuning
best_ntree <- best_params$ntree
best_mtry <- best_params$mtry
best_nodesize <- best_params$nodesize
best_maxnodes <- best_params$maxnodes
best_maxdepth <- best_params$maxdepth

# Definisikan formula
formula_rf <- as.formula("Y ~ .")

# Train model dengan best_params
set.seed(123)  # Untuk reprodusibilitas
rf_model <- randomForest(formula_rf, data = train_data,
                         ntree = best_ntree,       # Best ntree
                         mtry = best_mtry,        # Best mtry
                         nodesize = best_nodesize, # Best nodesize
                         maxnodes = best_maxnodes, # Best maxnodes
                         sampsize = 0.8 * nrow(train_data), # Bootstrap sample size
                         importance = TRUE)   # Melihat pentingnya variabel

# Print model
print(rf_model)
```

```{r}
pred_rf <- predict(rf_model, test_data)
mse_rf <- mean((y_test - pred_rf)^2)
rsq_rf <- 1 - sum((y_test - pred_rf)^2) / sum((y_test - mean(y_test))^2)
```

```{r}
# Plot feature importance
varImpPlot(rf_model, main = "Feature Importance in Random Forest")
```

## AdaBoost (Klasifikasi)
```{r}
library(adabag)
library(caret)
```

## GBM
```{r}
library(gbm)
library(caret)
```


```{r}
# Set grid untuk tuning
tune_grid <- expand.grid(
  n.trees = c(100, 300, 500),        # Jumlah pohon
  interaction.depth = c(1, 3, 5),     # Kedalaman pohon
  shrinkage = c(0.01, 0.05, 0.1),     # Learning rate
  n.minobsinnode = c(5, 10, 15)       # Minimum sampel per daun
)

# Set kontrol cross-validation
train_control <- trainControl(
  method = "cv", 
  number = 5, 
  verboseIter = TRUE
)

# Tuning model
set.seed(123)
gbm_tuned <- train(
  Y ~ ., 
  data = train_data, 
  method = "gbm", 
  trControl = train_control,
  tuneGrid = tune_grid,
  verbose = FALSE
)

# Best hyperparameters
best_params <- gbm_tuned$bestTune
print(best_params)

```

```{r}
set.seed(123)
gbm_best <- gbm(
  Y ~ ., 
  data = train_data, 
  distribution = "gaussian",
  n.trees = best_params$n.trees,
  interaction.depth = best_params$interaction.depth,
  shrinkage = best_params$shrinkage,
  n.minobsinnode = best_params$n.minobsinnode
)
```


```{r}
pred_gbm_best <- predict(gbm_best, newdata = test_data, n.trees = best_params$n.trees)
mse_gbm_best <- mean((test_data$Y - pred_gbm_best)^2)
rsq_gbm_best <- 1 - sum((test_data$Y - pred_gbm_best)^2) / sum((test_data$Y - mean(test_data$Y))^2)
```


## Neural Network
```{r}
library(nnet)
```


```{r}
nn_model <- nnet(Y ~ ., data = train_data, 
                 size = 5,  # Jumlah neuron di hidden layer
                 linout = TRUE,  # Karena ini regresi (bukan klasifikasi)
                 decay = 0.01,   # Regularisasi (L2 penalty)
                 maxit = 500)    # Iterasi training

summary(nn_model)
```

```{r}
library(NeuralNetTools)
plotnet(nn_model)
```


```{r}
pred_nn <- predict(nn_model, test_data)
mse_nn <- mean((test_data$Y - pred_nn)^2)
rsq_nn <- 1 - sum((test_data$Y - pred_nn)^2) / sum((test_data$Y - mean(test_data$Y))^2)
```

# Summary
```{r}
# Membuat Data Frame Ringkasan MSE dan R-squared
model_summary <- data.frame(
  Model = c("Linear Regression", "Forward Selection", "Backward Elimination", 
            "Stepwise Regression", "Best Subset Selection", "Ridge Regression", 
            "LASSO Regression", "Elastic Net", "Polynomial Regression", 
            "Piecewise Regression", "Spline Regression", "GAM", 
            "Regression Tree", "Pruned Regression Tree", "Random Forest",
            "GBM", "Neural Network"),
  MSE = c(mse_lm, mse_forward, mse_backward, mse_stepwise, mse_best_subset, 
          mse_ridge, mse_lasso, mse_en, mse_poly, mse_piecewise, 
          mse_spline, mse_gam, mse_tree, mse_pruned, mse_rf, mse_gbm_best,
          mse_nn),
  R_squared = c(rsq_lm, rsq_forward, rsq_backward, rsq_stepwise, rsq_best_subset, 
                rsq_ridge, rsq_lasso, rsq_en, rsq_poly, rsq_piecewise, 
                rsq_spline, rsq_gam, rsq_tree, rsq_pruned, rsq_rf, rsq_gbm_best,
                rsq_nn)
)

# Membulatkan hingga tiga angka di belakang koma
model_summary$MSE <- round(model_summary$MSE, 3)
model_summary$R_squared <- round(model_summary$R_squared, 3)

# Menampilkan tabel
print(model_summary, row.names = FALSE)

```

